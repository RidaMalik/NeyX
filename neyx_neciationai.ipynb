{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "neyx-neciationai.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RidaMalik/NeyX/blob/master/neyx_neciationai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "q-zmC--hGOLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBLjmT5WGOL5",
        "colab_type": "text"
      },
      "source": [
        "## Plan of notebook:\n",
        "1. Clone the GitHub repository of google-research\n",
        "2. Import all modules and libraries needed for the notebook.\n",
        "3. BERT model parameters definition.\n",
        "4. Dataset builder.\n",
        "5. Model Building: class and functions.\n",
        "6. Model definition: define our model instance.\n",
        "7. Function to Create dialog samples from directory and save it into file.\n",
        "8. Function to Create schema embeddings and save it into file.\n",
        "9. Main function.\n",
        "10. Running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMKCvzvNGOL9",
        "colab_type": "text"
      },
      "source": [
        "# 1. Clone the GitHub repository of google-research\n",
        "We have to clone the repository because we need some module from this repo which contain some necessary function for our system. So we have to import these modules from `schema_guided_dst` folder. That's the folder which contain all we need for our projet. We have to understand and adapt it in our case.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "7hXSbmRHGOL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ea48e80b-3b02-42a3-d739-401c10dc1751"
      },
      "source": [
        "!git clone https://github.com/google-research/google-research/blob/master/schema_guided_dst/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'schema_guided_dst'...\n",
            "fatal: repository 'https://github.com/google-research/google-research/blob/master/schema_guided_dst/' not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "n5aIDlM_GOMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "750141fc-2ff5-4c96-8f07-c3b0afd8bed1"
      },
      "source": [
        "!pip install git+https://github.com/google-research/google-research"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/google-research/google-research\n",
            "  Cloning https://github.com/google-research/google-research to /tmp/pip-req-build-zxed63v_\n",
            "  Running command git clone -q https://github.com/google-research/google-research /tmp/pip-req-build-zxed63v_\n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLl5OYaJGOMN",
        "colab_type": "text"
      },
      "source": [
        "# 2. Import all modules and libraries needed for the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AyFJUBj-GOMO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "f64fea11-9c2a-4d30-f77b-ad7a2afd5b28"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from schema_guided_dst import schema\n",
        "from schema_guided_dst.baseline import config\n",
        "from schema_guided_dst.baseline import data_utils\n",
        "from schema_guided_dst.baseline import extract_schema_embedding\n",
        "from schema_guided_dst.baseline import pred_utils\n",
        "from schema_guided_dst.baseline.bert import modeling\n",
        "from schema_guided_dst.baseline.bert import optimization\n",
        "from schema_guided_dst.baseline.bert import tokenization\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3326f7d6714e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mschema_guided_dst\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mschema_guided_dst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mschema_guided_dst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'schema_guided_dst'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORzfWVf1GOMV",
        "colab_type": "text"
      },
      "source": [
        "# 3. BERT model parameters definition\n",
        "We define some parameters of our model as `flags` style way. It's the structure to define `training parameters` of BERT and many others transformers models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SCpCyPEuGOMX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "180aeef7-060d-4c35-fe52-7cc0ee3d4eec"
      },
      "source": [
        "flags = tf.compat.v1.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# BERT based utterance encoder related flags.\n",
        "flags.DEFINE_string(\"bert_ckpt_dir\", None,\n",
        "                    \"Directory containing pre-trained BERT checkpoint.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"do_lower_case\", False,\n",
        "    \"Whether to lower case the input text. Should be True for uncased \"\n",
        "    \"models and False for cased models.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_seq_length\", 80,\n",
        "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
        "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
        "    \"than this will be padded.\")\n",
        "\n",
        "flags.DEFINE_float(\"dropout_rate\", 0.1,\n",
        "                   \"Dropout rate for BERT representations.\")\n",
        "\n",
        "# Hyperparameters and optimization related flags.\n",
        "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n",
        "\n",
        "flags.DEFINE_integer(\"predict_batch_size\", 8, \"Total batch size for predict.\")\n",
        "\n",
        "flags.DEFINE_float(\"learning_rate\", 1e-4, \"The initial learning rate for Adam.\")\n",
        "\n",
        "flags.DEFINE_float(\"num_train_epochs\", 80.0,\n",
        "                   \"Total number of training epochs to perform.\")\n",
        "flags.DEFINE_float(\n",
        "    \"warmup_proportion\", 0.1,\n",
        "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
        "    \"E.g., 0.1 = 10% of training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
        "                     \"How often to save the model checkpoint.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"tpu_name\", None,\n",
        "    \"The Cloud TPU to use for training. This should be either the name \"\n",
        "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
        "    \"url.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"tpu_zone\", None,\n",
        "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"gcp_project\", None,\n",
        "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"num_tpu_cores\", 8,\n",
        "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"use_one_hot_embeddings\", False,\n",
        "    \"If True, tf.one_hot will be used for embedding lookups, otherwise \"\n",
        "    \"tf.nn.embedding_lookup will be used. On TPUs, this should be True \"\n",
        "    \"since it is much faster.\")\n",
        "\n",
        "# Input and output paths and other flags.\n",
        "flags.DEFINE_enum(\"task_name\", None, config.DATASET_CONFIG.keys(),\n",
        "                  \"The name of the task to train.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"dstc8_data_dir\", None,\n",
        "    \"Directory for the downloaded DSTC8 data, which contains the dialogue files\"\n",
        "    \" and schema files of all datasets (eg train, dev)\")\n",
        "\n",
        "flags.DEFINE_enum(\"run_mode\", None, [\"train\", \"predict\"],\n",
        "                  \"The mode to run the script in.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"output_dir\", None,\n",
        "    \"The output directory where the model checkpoints will be written.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"schema_embedding_dir\", None,\n",
        "    \"Directory where .npy file for embedding of entities (slots, values,\"\n",
        "    \" intents) in the dataset_split's schema are stored.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"dialogues_example_dir\", None,\n",
        "    \"Directory where tf.record of DSTC8 dialogues data are stored.\")\n",
        "\n",
        "flags.DEFINE_enum(\"dataset_split\", None, [\"train\", \"dev\", \"test\"],\n",
        "                  \"Dataset split for training / prediction.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"eval_ckpt\", \"\",\n",
        "    \"Comma separated numbers, each being a step number of model checkpoint\"\n",
        "    \" which makes predictions.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"overwrite_dial_file\", False,\n",
        "    \"Whether to generate a new Tf.record file saving the dialogue examples.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"overwrite_schema_emb_file\", False,\n",
        "    \"Whether to generate a new schema_emb file saving the schemas' embeddings.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"log_data_warnings\", False,\n",
        "    \"If True, warnings created using data processing are logged.\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6bf8122f1908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Input and output paths and other flags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m flags.DEFINE_enum(\"task_name\", None, config.DATASET_CONFIG.keys(),\n\u001b[0m\u001b[1;32m     75\u001b[0m                   \"The name of the task to train.\")\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsAoNMakGOMe",
        "colab_type": "text"
      },
      "source": [
        "# 4. Dataset builder.\n",
        "Here is a function to build batch of dataset sample as input to pass to our model for training. It take a single `dialogue file` as an input with `schema file` path which is embedded before (both two are path to .json file). It also take a dataset config objet to build our input.\n",
        "\n",
        "**Note** that the aim of this function is to build a `tensorFlow dataset objet` to pass to our model. Because training require that we pass it as `input` to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4jDk9RqaGOMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modified from run_classifier.file_based_input_fn_builder\n",
        "def _file_based_input_fn_builder(dataset_config, input_dial_file,\n",
        "                                 schema_embedding_file, is_training,\n",
        "                                 drop_remainder):\n",
        "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "    max_num_cat_slot = dataset_config.max_num_cat_slot\n",
        "    max_num_noncat_slot = dataset_config.max_num_noncat_slot\n",
        "    max_num_total_slot = max_num_cat_slot + max_num_noncat_slot\n",
        "    max_num_intent = dataset_config.max_num_intent\n",
        "    max_utt_len = FLAGS.max_seq_length\n",
        "\n",
        "    name_to_features = {\n",
        "        \"example_id\":\n",
        "          tf.io.FixedLenFeature([], tf.string),\n",
        "        \"is_real_example\":\n",
        "          tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"service_id\":\n",
        "          tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"utt\":\n",
        "          tf.io.FixedLenFeature([max_utt_len], tf.int64),\n",
        "        \"utt_mask\":\n",
        "          tf.io.FixedLenFeature([max_utt_len], tf.int64),\n",
        "        \"utt_seg\":\n",
        "          tf.io.FixedLenFeature([max_utt_len], tf.int64),\n",
        "        \"cat_slot_num\":\n",
        "          tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"cat_slot_status\":\n",
        "          tf.io.FixedLenFeature([max_num_cat_slot], tf.int64),\n",
        "        \"cat_slot_value_num\":\n",
        "          tf.io.FixedLenFeature([max_num_cat_slot], tf.int64),\n",
        "        \"cat_slot_value\":\n",
        "          tf.io.FixedLenFeature([max_num_cat_slot], tf.int64),\n",
        "        \"noncat_slot_num\":\n",
        "          tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"noncat_slot_status\":\n",
        "          tf.io.FixedLenFeature([max_num_noncat_slot], tf.int64),\n",
        "        \"noncat_slot_value_start\":\n",
        "          tf.io.FixedLenFeature([max_num_noncat_slot], tf.int64),\n",
        "        \"noncat_slot_value_end\":\n",
        "          tf.io.FixedLenFeature([max_num_noncat_slot], tf.int64),\n",
        "        \"noncat_alignment_start\":\n",
        "          tf.io.FixedLenFeature([max_utt_len], tf.int64),\n",
        "        \"noncat_alignment_end\":\n",
        "          tf.io.FixedLenFeature([max_utt_len], tf.int64),\n",
        "        \"req_slot_num\":\n",
        "          tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"req_slot_status\":\n",
        "          tf.io.FixedLenFeature([max_num_total_slot], tf.int64),\n",
        "        \"intent_num\":\n",
        "          tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"intent_status\":\n",
        "          tf.io.FixedLenFeature([max_num_intent], tf.int64),\n",
        "    }\n",
        "    with tf.io.gfile.GFile(schema_embedding_file, \"rb\") as f:\n",
        "        schema_data = np.load(f, allow_pickle=True)\n",
        "\n",
        "    # Convert from list of dict to dict of list\n",
        "    schema_data_dict = collections.defaultdict(list)\n",
        "    for service in schema_data:\n",
        "        schema_data_dict[\"cat_slot_emb\"].append(service[\"cat_slot_emb\"])\n",
        "        schema_data_dict[\"cat_slot_value_emb\"].append(service[\"cat_slot_value_emb\"])\n",
        "        schema_data_dict[\"noncat_slot_emb\"].append(service[\"noncat_slot_emb\"])\n",
        "        schema_data_dict[\"req_slot_emb\"].append(service[\"req_slot_emb\"])\n",
        "        schema_data_dict[\"intent_emb\"].append(service[\"intent_emb\"])\n",
        "\n",
        "    def _decode_record(record, name_to_features, schema_tensors):\n",
        "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "\n",
        "        example = tf.parse_single_example(record, name_to_features)\n",
        "\n",
        "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "        # So cast all int64 to int32.\n",
        "        for name in list(example.keys()):\n",
        "            t = example[name]\n",
        "            if t.dtype == tf.int64:\n",
        "                t = tf.cast(t, tf.int32)\n",
        "            example[name] = t\n",
        "\n",
        "        # Here we need to insert schema's entity embedding to each example.\n",
        "\n",
        "        # Shapes for reference: (all have type tf.float32)\n",
        "        # \"cat_slot_emb\": [max_num_cat_slot, hidden_dim]\n",
        "        # \"cat_slot_value_emb\": [max_num_cat_slot, max_num_value, hidden_dim]\n",
        "        # \"noncat_slot_emb\": [max_num_noncat_slot, hidden_dim]\n",
        "        # \"req_slot_emb\": [max_num_total_slot, hidden_dim]\n",
        "        # \"intent_emb\": [max_num_intent, hidden_dim]\n",
        "\n",
        "        service_id = example[\"service_id\"]\n",
        "        for key, value in schema_tensors.items():\n",
        "            example[key] = value[service_id]\n",
        "        return example\n",
        "\n",
        "    def input_fn(params):\n",
        "        \"\"\"The actual input function.\"\"\"\n",
        "        batch_size = params[\"batch_size\"]\n",
        "\n",
        "        # For training, we want a lot of parallel reading and shuffling.\n",
        "        # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "        d = tf.data.TFRecordDataset(input_dial_file)\n",
        "        # Uncomment for debugging\n",
        "        # d = d.take(12)\n",
        "        if is_training:\n",
        "            d = d.repeat()\n",
        "            d = d.shuffle(buffer_size=100)\n",
        "        schema_tensors = {}\n",
        "        for key, array in schema_data_dict.items():\n",
        "            schema_tensors[key] = tf.convert_to_tensor(np.asarray(array, np.float32))\n",
        "\n",
        "        d = d.apply(\n",
        "            tf.data.experimental.map_and_batch(\n",
        "                lambda rec: _decode_record(rec, name_to_features, schema_tensors),\n",
        "                batch_size=batch_size,\n",
        "                drop_remainder=drop_remainder))\n",
        "        return d\n",
        "\n",
        "    return input_fn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1WA5JAtGOMo",
        "colab_type": "text"
      },
      "source": [
        "# 5. Model Building: class and functions.\n",
        "Here is a class for our model construction. It contain all we need to manipulate for training. In this class:\n",
        "* we have function for model definition (aka it architecture with input, computation an output) `define_model` function; \n",
        "* We also have function to calculate loss for our model on a batch of sample `define_loss`; \n",
        "* We alsos a function to make predictions on a batch of sample `define_predictions` function;\n",
        "* We also have others fonctions for utterance encoding, slot request, categorical slot tracking, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TpBLk17cGOMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SchemaGuidedDST(object):\n",
        "    \"\"\"Baseline model for schema guided dialogue state tracking.\"\"\"\n",
        "\n",
        "    def __init__(self, bert_config, use_one_hot_embeddings):\n",
        "        self._bert_config = bert_config\n",
        "        self._use_one_hot_embeddings = use_one_hot_embeddings\n",
        "\n",
        "    def define_model(self, features, is_training):\n",
        "        \"\"\"Define the model computation.\n",
        "        Args:\n",
        "          features: A dict mapping feature names to corresponding tensors.\n",
        "          is_training: A boolean which is True when the model is being trained.\n",
        "        Returns:\n",
        "          outputs: A dict mapping output names to corresponding tensors.\n",
        "        \"\"\"\n",
        "        # Encode the utterances using BERT.\n",
        "        self._encoded_utterance, self._encoded_tokens = (\n",
        "            self._encode_utterances(features, is_training))\n",
        "        outputs = {}\n",
        "        outputs[\"logit_intent_status\"] = self._get_intents(features)\n",
        "        outputs[\"logit_req_slot_status\"] = self._get_requested_slots(features)\n",
        "        cat_slot_status, cat_slot_value = self._get_categorical_slot_goals(features)\n",
        "        outputs[\"logit_cat_slot_status\"] = cat_slot_status\n",
        "        outputs[\"logit_cat_slot_value\"] = cat_slot_value\n",
        "        noncat_slot_status, noncat_span_start, noncat_span_end = (\n",
        "            self._get_noncategorical_slot_goals(features))\n",
        "        outputs[\"logit_noncat_slot_status\"] = noncat_slot_status\n",
        "        outputs[\"logit_noncat_slot_start\"] = noncat_span_start\n",
        "        outputs[\"logit_noncat_slot_end\"] = noncat_span_end\n",
        "        return outputs\n",
        "\n",
        "    def define_loss(self, features, outputs):\n",
        "        \"\"\"Obtain the loss of the model.\"\"\"\n",
        "        # Intents.\n",
        "        # Shape: (batch_size, max_num_intents + 1).\n",
        "        intent_logits = outputs[\"logit_intent_status\"]\n",
        "        # Shape: (batch_size, max_num_intents).\n",
        "        intent_labels = features[\"intent_status\"]\n",
        "        # Add label corresponding to NONE intent.\n",
        "        num_active_intents = tf.expand_dims(\n",
        "            tf.reduce_sum(intent_labels, axis=1), axis=1)\n",
        "        none_intent_label = tf.ones_like(num_active_intents) - num_active_intents\n",
        "        # Shape: (batch_size, max_num_intents + 1).\n",
        "        onehot_intent_labels = tf.concat([none_intent_label, intent_labels], axis=1)\n",
        "        intent_loss = tf.losses.softmax_cross_entropy(\n",
        "            onehot_intent_labels,\n",
        "            intent_logits,\n",
        "            weights=features[\"is_real_example\"])\n",
        "\n",
        "        # Requested slots.\n",
        "        # Shape: (batch_size, max_num_slots).\n",
        "        requested_slot_logits = outputs[\"logit_req_slot_status\"]\n",
        "        requested_slot_labels = features[\"req_slot_status\"]\n",
        "        max_num_requested_slots = requested_slot_labels.get_shape().as_list()[-1]\n",
        "        weights = tf.sequence_mask(\n",
        "            features[\"req_slot_num\"], maxlen=max_num_requested_slots)\n",
        "        # Sigmoid cross entropy is used because more than one slots can be requested\n",
        "        # in a single utterance.\n",
        "        requested_slot_loss = tf.losses.sigmoid_cross_entropy(\n",
        "            requested_slot_labels, requested_slot_logits, weights=weights)\n",
        "\n",
        "        # Categorical slot status.\n",
        "        # Shape: (batch_size, max_num_cat_slots, 3).\n",
        "        cat_slot_status_logits = outputs[\"logit_cat_slot_status\"]\n",
        "        cat_slot_status_labels = features[\"cat_slot_status\"]\n",
        "        max_num_cat_slots = cat_slot_status_labels.get_shape().as_list()[-1]\n",
        "        one_hot_labels = tf.one_hot(cat_slot_status_labels, 3, dtype=tf.int32)\n",
        "        cat_weights = tf.sequence_mask(\n",
        "            features[\"cat_slot_num\"], maxlen=max_num_cat_slots, dtype=tf.float32)\n",
        "        cat_slot_status_loss = tf.losses.softmax_cross_entropy(\n",
        "            tf.reshape(one_hot_labels, [-1, 3]),\n",
        "            tf.reshape(cat_slot_status_logits, [-1, 3]),\n",
        "            weights=tf.reshape(cat_weights, [-1]))\n",
        "\n",
        "        # Categorical slot values.\n",
        "        # Shape: (batch_size, max_num_cat_slots, max_num_slot_values).\n",
        "        cat_slot_value_logits = outputs[\"logit_cat_slot_value\"]\n",
        "        cat_slot_value_labels = features[\"cat_slot_value\"]\n",
        "        max_num_slot_values = cat_slot_value_logits.get_shape().as_list()[-1]\n",
        "        one_hot_labels = tf.one_hot(\n",
        "            cat_slot_value_labels, max_num_slot_values, dtype=tf.int32)\n",
        "        # Zero out losses for categorical slot value when the slot status is not\n",
        "        # active.\n",
        "        cat_loss_weight = tf.cast(\n",
        "            tf.equal(cat_slot_status_labels, data_utils.STATUS_ACTIVE), tf.float32)\n",
        "        cat_slot_value_loss = tf.losses.softmax_cross_entropy(\n",
        "            tf.reshape(one_hot_labels, [-1, max_num_slot_values]),\n",
        "            tf.reshape(cat_slot_value_logits, [-1, max_num_slot_values]),\n",
        "            weights=tf.reshape(cat_weights * cat_loss_weight, [-1]))\n",
        "\n",
        "        # Non-categorical slot status.\n",
        "        # Shape: (batch_size, max_num_noncat_slots, 3).\n",
        "        noncat_slot_status_logits = outputs[\"logit_noncat_slot_status\"]\n",
        "        noncat_slot_status_labels = features[\"noncat_slot_status\"]\n",
        "        max_num_noncat_slots = noncat_slot_status_labels.get_shape().as_list()[-1]\n",
        "        one_hot_labels = tf.one_hot(noncat_slot_status_labels, 3, dtype=tf.int32)\n",
        "        noncat_weights = tf.sequence_mask(\n",
        "            features[\"noncat_slot_num\"],\n",
        "            maxlen=max_num_noncat_slots,\n",
        "            dtype=tf.float32)\n",
        "        # Logits for padded (invalid) values are already masked.\n",
        "        noncat_slot_status_loss = tf.losses.softmax_cross_entropy(\n",
        "            tf.reshape(one_hot_labels, [-1, 3]),\n",
        "            tf.reshape(noncat_slot_status_logits, [-1, 3]),\n",
        "            weights=tf.reshape(noncat_weights, [-1]))\n",
        "\n",
        "        # Non-categorical slot spans.\n",
        "        # Shape: (batch_size, max_num_noncat_slots, max_num_tokens).\n",
        "        span_start_logits = outputs[\"logit_noncat_slot_start\"]\n",
        "        span_start_labels = features[\"noncat_slot_value_start\"]\n",
        "        max_num_tokens = span_start_logits.get_shape().as_list()[-1]\n",
        "        onehot_start_labels = tf.one_hot(\n",
        "            span_start_labels, max_num_tokens, dtype=tf.int32)\n",
        "        # Shape: (batch_size, max_num_noncat_slots, max_num_tokens).\n",
        "        span_end_logits = outputs[\"logit_noncat_slot_end\"]\n",
        "        span_end_labels = features[\"noncat_slot_value_end\"]\n",
        "        onehot_end_labels = tf.one_hot(\n",
        "            span_end_labels, max_num_tokens, dtype=tf.int32)\n",
        "        # Zero out losses for non-categorical slot spans when the slot status is not\n",
        "        # active.\n",
        "        noncat_loss_weight = tf.cast(\n",
        "            tf.equal(noncat_slot_status_labels, data_utils.STATUS_ACTIVE),\n",
        "            tf.float32)\n",
        "        span_start_loss = tf.losses.softmax_cross_entropy(\n",
        "            tf.reshape(onehot_start_labels, [-1, max_num_tokens]),\n",
        "            tf.reshape(span_start_logits, [-1, max_num_tokens]),\n",
        "            weights=tf.reshape(noncat_weights * noncat_loss_weight, [-1]))\n",
        "        span_end_loss = tf.losses.softmax_cross_entropy(\n",
        "            tf.reshape(onehot_end_labels, [-1, max_num_tokens]),\n",
        "            tf.reshape(span_end_logits, [-1, max_num_tokens]),\n",
        "            weights=tf.reshape(noncat_weights * noncat_loss_weight, [-1]))\n",
        "\n",
        "        losses = {\n",
        "            \"intent_loss\": intent_loss,\n",
        "            \"requested_slot_loss\": requested_slot_loss,\n",
        "            \"cat_slot_status_loss\": cat_slot_status_loss,\n",
        "            \"cat_slot_value_loss\": cat_slot_value_loss,\n",
        "            \"noncat_slot_status_loss\": noncat_slot_status_loss,\n",
        "            \"span_start_loss\": span_start_loss,\n",
        "            \"span_end_loss\": span_end_loss,\n",
        "        }\n",
        "        for loss_name, loss in losses.items():\n",
        "            tf.summary.scalar(loss_name, loss)\n",
        "        return sum(losses.values()) / len(losses)\n",
        "\n",
        "    def define_predictions(self, features, outputs):\n",
        "        \"\"\"Define model predictions.\"\"\"\n",
        "        predictions = {\n",
        "            \"example_id\": features[\"example_id\"],\n",
        "            \"service_id\": features[\"service_id\"],\n",
        "            \"is_real_example\": features[\"is_real_example\"],\n",
        "        }\n",
        "        # Scores are output for each intent.\n",
        "        # Note that the intent indices are shifted by 1 to account for NONE intent.\n",
        "        predictions[\"intent_status\"] = tf.argmax(\n",
        "            outputs[\"logit_intent_status\"], axis=-1)\n",
        "\n",
        "        # Scores are output for each requested slot.\n",
        "        predictions[\"req_slot_status\"] = tf.sigmoid(\n",
        "            outputs[\"logit_req_slot_status\"])\n",
        "\n",
        "        # For categorical slots, the status of each slot and the predicted value are\n",
        "        # output.\n",
        "        predictions[\"cat_slot_status\"] = tf.argmax(\n",
        "            outputs[\"logit_cat_slot_status\"], axis=-1)\n",
        "        predictions[\"cat_slot_value\"] = tf.argmax(\n",
        "            outputs[\"logit_cat_slot_value\"], axis=-1)\n",
        "\n",
        "        # For non-categorical slots, the status of each slot and the indices for\n",
        "        # spans are output.\n",
        "        predictions[\"noncat_slot_status\"] = tf.argmax(\n",
        "            outputs[\"logit_noncat_slot_status\"], axis=-1)\n",
        "        start_scores = tf.nn.softmax(outputs[\"logit_noncat_slot_start\"], axis=-1)\n",
        "        end_scores = tf.nn.softmax(outputs[\"logit_noncat_slot_end\"], axis=-1)\n",
        "        _, max_num_slots, max_num_tokens = end_scores.get_shape().as_list()\n",
        "        batch_size = tf.shape(end_scores)[0]\n",
        "        # Find the span with the maximum sum of scores for start and end indices.\n",
        "        total_scores = (\n",
        "            tf.expand_dims(start_scores, axis=3) +\n",
        "            tf.expand_dims(end_scores, axis=2))\n",
        "        # Mask out scores where start_index > end_index.\n",
        "        start_idx = tf.reshape(tf.range(max_num_tokens), [1, 1, -1, 1])\n",
        "        end_idx = tf.reshape(tf.range(max_num_tokens), [1, 1, 1, -1])\n",
        "        invalid_index_mask = tf.tile((start_idx > end_idx),\n",
        "                                     [batch_size, max_num_slots, 1, 1])\n",
        "        total_scores = tf.where(invalid_index_mask, tf.zeros_like(total_scores),\n",
        "                                total_scores)\n",
        "        max_span_index = tf.argmax(\n",
        "            tf.reshape(total_scores, [-1, max_num_slots, max_num_tokens**2]),\n",
        "            axis=-1)\n",
        "        span_start_index = tf.floordiv(max_span_index, max_num_tokens)\n",
        "        span_end_index = tf.floormod(max_span_index, max_num_tokens)\n",
        "        predictions[\"noncat_slot_start\"] = span_start_index\n",
        "        predictions[\"noncat_slot_end\"] = span_end_index\n",
        "        # Add inverse alignments.\n",
        "        predictions[\"noncat_alignment_start\"] = features[\"noncat_alignment_start\"]\n",
        "        predictions[\"noncat_alignment_end\"] = features[\"noncat_alignment_end\"]\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def _encode_utterances(self, features, is_training):\n",
        "        \"\"\"Encode system and user utterances using BERT.\"\"\"\n",
        "        # Optain the embedded representation of system and user utterances in the\n",
        "        # turn and the corresponding token level representations.\n",
        "        bert_encoder = modeling.BertModel(\n",
        "            config=self._bert_config,\n",
        "            is_training=is_training,\n",
        "            input_ids=features[\"utt\"],\n",
        "            input_mask=features[\"utt_mask\"],\n",
        "            token_type_ids=features[\"utt_seg\"],\n",
        "            use_one_hot_embeddings=self._use_one_hot_embeddings)\n",
        "        encoded_utterance = bert_encoder.get_pooled_output()\n",
        "        encoded_tokens = bert_encoder.get_sequence_output()\n",
        "\n",
        "        # Apply dropout in training mode.\n",
        "        encoded_utterance = tf.layers.dropout(\n",
        "            encoded_utterance, rate=FLAGS.dropout_rate, training=is_training)\n",
        "        encoded_tokens = tf.layers.dropout(\n",
        "            encoded_tokens, rate=FLAGS.dropout_rate, training=is_training)\n",
        "        return encoded_utterance, encoded_tokens\n",
        "\n",
        "    def _get_logits(self, element_embeddings, num_classes, name_scope):\n",
        "        \"\"\"Get logits for elements by conditioning on utterance embedding.\n",
        "        Args:\n",
        "          element_embeddings: A tensor of shape (batch_size, num_elements,\n",
        "            embedding_dim).\n",
        "          num_classes: An int containing the number of classes for which logits are\n",
        "            to be generated.\n",
        "          name_scope: The name scope to be used for layers.\n",
        "        Returns:\n",
        "          A tensor of shape (batch_size, num_elements, num_classes) containing the\n",
        "          logits.\n",
        "        \"\"\"\n",
        "        _, num_elements, embedding_dim = element_embeddings.get_shape().as_list()\n",
        "        # Project the utterance embeddings.\n",
        "        utterance_proj = tf.keras.layers.Dense(\n",
        "            units=embedding_dim,\n",
        "            activation=modeling.gelu,\n",
        "            name=\"{}_utterance_proj\".format(name_scope))\n",
        "        utterance_embedding = utterance_proj(self._encoded_utterance)\n",
        "        # Combine the utterance and element embeddings.\n",
        "        repeat_utterance_embeddings = tf.tile(\n",
        "            tf.expand_dims(utterance_embedding, axis=1), [1, num_elements, 1])\n",
        "        utterance_element_emb = tf.concat(\n",
        "            [repeat_utterance_embeddings, element_embeddings], axis=2)\n",
        "        # Project the combined embeddings to obtain logits.\n",
        "        layer_1 = tf.keras.layers.Dense(\n",
        "            units=embedding_dim,\n",
        "            activation=modeling.gelu,\n",
        "            name=\"{}_projection_1\".format(name_scope))\n",
        "        layer_2 = tf.keras.layers.Dense(\n",
        "            units=num_classes, name=\"{}_projection_2\".format(name_scope))\n",
        "        return layer_2(layer_1(utterance_element_emb))\n",
        "\n",
        "    def _get_intents(self, features):\n",
        "        \"\"\"Obtain logits for intents.\"\"\"\n",
        "        intent_embeddings = features[\"intent_emb\"]\n",
        "        # Add a trainable vector for the NONE intent.\n",
        "        _, max_num_intents, embedding_dim = intent_embeddings.get_shape().as_list()\n",
        "        null_intent_embedding = tf.get_variable(\n",
        "            \"null_intent_embedding\",\n",
        "            shape=[1, 1, embedding_dim],\n",
        "            initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "        batch_size = tf.shape(intent_embeddings)[0]\n",
        "        repeated_null_intent_embedding = tf.tile(null_intent_embedding,\n",
        "                                                 [batch_size, 1, 1])\n",
        "        intent_embeddings = tf.concat(\n",
        "            [repeated_null_intent_embedding, intent_embeddings], axis=1)\n",
        "\n",
        "        logits = self._get_logits(intent_embeddings, 1, \"intents\")\n",
        "        # Shape: (batch_size, max_intents + 1)\n",
        "        logits = tf.squeeze(logits, axis=-1)\n",
        "        # Mask out logits for padded intents. 1 is added to account for NONE intent.\n",
        "        mask = tf.sequence_mask(\n",
        "            features[\"intent_num\"] + 1, maxlen=max_num_intents + 1)\n",
        "        negative_logits = -0.7 * tf.ones_like(logits) * logits.dtype.max\n",
        "        return tf.where(mask, logits, negative_logits)\n",
        "\n",
        "    def _get_requested_slots(self, features):\n",
        "        \"\"\"Obtain logits for requested slots.\"\"\"\n",
        "        slot_embeddings = features[\"req_slot_emb\"]\n",
        "        logits = self._get_logits(slot_embeddings, 1, \"requested_slots\")\n",
        "        return tf.squeeze(logits, axis=-1)\n",
        "\n",
        "    def _get_categorical_slot_goals(self, features):\n",
        "        \"\"\"Obtain logits for status and values for categorical slots.\"\"\"\n",
        "        # Predict the status of all categorical slots.\n",
        "        slot_embeddings = features[\"cat_slot_emb\"]\n",
        "        status_logits = self._get_logits(slot_embeddings, 3,\n",
        "                                         \"categorical_slot_status\")\n",
        "\n",
        "        # Predict the goal value.\n",
        "\n",
        "        # Shape: (batch_size, max_categorical_slots, max_categorical_values,\n",
        "        # embedding_dim).\n",
        "        value_embeddings = features[\"cat_slot_value_emb\"]\n",
        "        _, max_num_slots, max_num_values, embedding_dim = (\n",
        "            value_embeddings.get_shape().as_list())\n",
        "        value_embeddings_reshaped = tf.reshape(\n",
        "            value_embeddings, [-1, max_num_slots * max_num_values, embedding_dim])\n",
        "        value_logits = self._get_logits(value_embeddings_reshaped, 1,\n",
        "                                        \"categorical_slot_values\")\n",
        "        # Reshape to obtain the logits for all slots.\n",
        "        value_logits = tf.reshape(value_logits, [-1, max_num_slots, max_num_values])\n",
        "        # Mask out logits for padded slots and values because they will be\n",
        "        # softmaxed.\n",
        "        mask = tf.sequence_mask(\n",
        "            features[\"cat_slot_value_num\"], maxlen=max_num_values)\n",
        "        negative_logits = -0.7 * tf.ones_like(value_logits) * value_logits.dtype.max\n",
        "        value_logits = tf.where(mask, value_logits, negative_logits)\n",
        "        return status_logits, value_logits\n",
        "\n",
        "    def _get_noncategorical_slot_goals(self, features):\n",
        "        \"\"\"Obtain logits for status and slot spans for non-categorical slots.\"\"\"\n",
        "        # Predict the status of all non-categorical slots.\n",
        "        slot_embeddings = features[\"noncat_slot_emb\"]\n",
        "        max_num_slots = slot_embeddings.get_shape().as_list()[1]\n",
        "        status_logits = self._get_logits(slot_embeddings, 3,\n",
        "                                         \"noncategorical_slot_status\")\n",
        "\n",
        "        # Predict the distribution for span indices.\n",
        "        token_embeddings = self._encoded_tokens\n",
        "        max_num_tokens = token_embeddings.get_shape().as_list()[1]\n",
        "        tiled_token_embeddings = tf.tile(\n",
        "            tf.expand_dims(token_embeddings, 1), [1, max_num_slots, 1, 1])\n",
        "        tiled_slot_embeddings = tf.tile(\n",
        "            tf.expand_dims(slot_embeddings, 2), [1, 1, max_num_tokens, 1])\n",
        "        # Shape: (batch_size, max_num_slots, max_num_tokens, 2 * embedding_dim).\n",
        "        slot_token_embeddings = tf.concat(\n",
        "            [tiled_slot_embeddings, tiled_token_embeddings], axis=3)\n",
        "\n",
        "        # Project the combined embeddings to obtain logits.\n",
        "        embedding_dim = slot_embeddings.get_shape().as_list()[-1]\n",
        "        layer_1 = tf.keras.layers.Dense(\n",
        "            units=embedding_dim,\n",
        "            activation=modeling.gelu,\n",
        "            name=\"noncat_spans_layer_1\")\n",
        "        layer_2 = tf.keras.layers.Dense(units=2, name=\"noncat_spans_layer_2\")\n",
        "        # Shape: (batch_size, max_num_slots, max_num_tokens, 2)\n",
        "        span_logits = layer_2(layer_1(slot_token_embeddings))\n",
        "\n",
        "        # Mask out invalid logits for padded tokens.\n",
        "        token_mask = features[\"utt_mask\"]  # Shape: (batch_size, max_num_tokens).\n",
        "        token_mask = tf.cast(token_mask, tf.bool)\n",
        "        tiled_token_mask = tf.tile(\n",
        "            tf.expand_dims(tf.expand_dims(token_mask, 1), 3),\n",
        "            [1, max_num_slots, 1, 2])\n",
        "        negative_logits = -0.7 * tf.ones_like(span_logits) * span_logits.dtype.max\n",
        "        span_logits = tf.where(tiled_token_mask, span_logits, negative_logits)\n",
        "        # Shape of both tensors: (batch_size, max_num_slots, max_num_tokens).\n",
        "        span_start_logits, span_end_logits = tf.unstack(span_logits, axis=3)\n",
        "        return status_logits, span_start_logits, span_end_logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M--4PQltGOMw",
        "colab_type": "text"
      },
      "source": [
        "# 6. Model definition: define our model instance.\n",
        "Here we configure an instace of our model class with training configuration. We make the model to be able for runing on TPU. So that is the definition of a function for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lnUDA4dCGOMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modified from run_classifier.model_fn_builder\n",
        "def _model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
        "                      num_train_steps, num_warmup_steps, use_tpu,\n",
        "                      use_one_hot_embeddings):\n",
        "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "        schema_guided_dst = SchemaGuidedDST(bert_config, use_one_hot_embeddings)\n",
        "        outputs = schema_guided_dst.define_model(features, is_training)\n",
        "        if is_training:\n",
        "            total_loss = schema_guided_dst.define_loss(features, outputs)\n",
        "        else:\n",
        "            total_loss = tf.constant(0.0)\n",
        "\n",
        "        tvars = tf.trainable_variables()\n",
        "        scaffold_fn = None\n",
        "        if init_checkpoint:\n",
        "            assignment_map, _ = modeling.get_assignment_map_from_checkpoint(\n",
        "              tvars, init_checkpoint)\n",
        "            if use_tpu:\n",
        "\n",
        "                def tpu_scaffold():\n",
        "                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "                    return tf.train.Scaffold()\n",
        "\n",
        "                scaffold_fn = tpu_scaffold\n",
        "            else:\n",
        "                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "        output_spec = None\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            train_op = optimization.create_optimizer(total_loss, learning_rate,\n",
        "                                                    num_train_steps,\n",
        "                                                    num_warmup_steps, use_tpu)\n",
        "            global_step = tf.train.get_or_create_global_step()\n",
        "            logged_tensors = {\n",
        "                  \"global_step\": global_step,\n",
        "                  \"total_loss\": total_loss,\n",
        "            }\n",
        "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "                  mode=mode,\n",
        "                  loss=total_loss,\n",
        "                  train_op=train_op,\n",
        "                  scaffold_fn=scaffold_fn,\n",
        "                  training_hooks=[\n",
        "                      tf.train.LoggingTensorHook(logged_tensors, every_n_iter=5)\n",
        "              ])\n",
        "\n",
        "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "                            mode=mode, loss=total_loss, scaffold_fn=scaffold_fn)\n",
        "\n",
        "        else:  # mode == tf.estimator.ModeKeys.PREDICT\n",
        "            predictions = schema_guided_dst.define_predictions(features, outputs)\n",
        "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "                             mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
        "\n",
        "        return output_spec\n",
        "\n",
        "    return model_fn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu8HKN2jGOM7",
        "colab_type": "text"
      },
      "source": [
        "# 7. Function to Create dialog samples from directory and save it into file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "msSY2Z59GOM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _create_dialog_examples(processor, dial_file):\n",
        "    \"\"\"Create dialog examples and save in the file.\"\"\"\n",
        "    if not tf.io.gfile.exists(FLAGS.dialogues_example_dir):\n",
        "        tf.io.gfile.makedirs(FLAGS.dialogues_example_dir)\n",
        "    frame_examples = processor.get_dialog_examples(FLAGS.dataset_split)\n",
        "    data_utils.file_based_convert_examples_to_features(frame_examples,\n",
        "                                                     processor.dataset_config,\n",
        "                                                     dial_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RttpaUUVGONH",
        "colab_type": "text"
      },
      "source": [
        "# 8. Function to Create schema embeddings and save it into file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ylnbWPWmGONJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _create_schema_embeddings(bert_config, schema_embedding_file,\n",
        "                              dataset_config):\n",
        "    \"\"\"Create schema embeddings and save it into file.\"\"\"\n",
        "    if not tf.io.gfile.exists(FLAGS.schema_embedding_dir):\n",
        "        tf.io.gfile.makedirs(FLAGS.schema_embedding_dir)\n",
        "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "    schema_emb_run_config = tf.contrib.tpu.RunConfig(\n",
        "      master=FLAGS.master,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          num_shards=FLAGS.num_tpu_cores,\n",
        "          per_host_input_for_training=is_per_host))\n",
        "\n",
        "    schema_json_path = os.path.join(FLAGS.dstc8_data_dir, FLAGS.dataset_split,\n",
        "                                  \"schema.json\")\n",
        "    schemas = schema.Schema(schema_json_path)\n",
        "\n",
        "    # Prepare BERT model for embedding a natural language descriptions.\n",
        "    bert_init_ckpt = os.path.join(FLAGS.bert_ckpt_dir, \"bert_model.ckpt\")\n",
        "    schema_emb_model_fn = extract_schema_embedding.model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=bert_init_ckpt,\n",
        "      use_tpu=FLAGS.use_tpu,\n",
        "      use_one_hot_embeddings=FLAGS.use_one_hot_embeddings)\n",
        "    # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "    # or GPU.\n",
        "    schema_emb_estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=FLAGS.use_tpu,\n",
        "      model_fn=schema_emb_model_fn,\n",
        "      config=schema_emb_run_config,\n",
        "      predict_batch_size=FLAGS.predict_batch_size)\n",
        "    vocab_file = os.path.join(FLAGS.bert_ckpt_dir, \"vocab.txt\")\n",
        "    tokenizer = tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
        "    emb_generator = extract_schema_embedding.SchemaEmbeddingGenerator(\n",
        "      tokenizer, schema_emb_estimator, FLAGS.max_seq_length)\n",
        "    emb_generator.save_embeddings(schemas, schema_embedding_file, dataset_config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-pRUiZ-GONQ",
        "colab_type": "text"
      },
      "source": [
        "# 9. Main function.\n",
        "This function is used for training and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XdIEf1IuGONS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def main(_):\n",
        "    vocab_file = os.path.join(FLAGS.bert_ckpt_dir, \"vocab.txt\")\n",
        "    task_name = FLAGS.task_name.lower()\n",
        "    if task_name not in config.DATASET_CONFIG:\n",
        "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
        "    dataset_config = config.DATASET_CONFIG[task_name]\n",
        "    processor = data_utils.Dstc8DataProcessor(\n",
        "      FLAGS.dstc8_data_dir,\n",
        "      dataset_config=dataset_config,\n",
        "      vocab_file=vocab_file,\n",
        "      do_lower_case=FLAGS.do_lower_case,\n",
        "      max_seq_length=FLAGS.max_seq_length,\n",
        "      log_data_warnings=FLAGS.log_data_warnings)\n",
        "\n",
        "    # Generate the dialogue examples if needed or specified.\n",
        "    dial_file_name = \"{}_{}_examples.tf_record\".format(task_name,\n",
        "                                                     FLAGS.dataset_split)\n",
        "    dial_file = os.path.join(FLAGS.dialogues_example_dir, dial_file_name)\n",
        "    if not tf.io.gfile.exists(dial_file) or FLAGS.overwrite_dial_file:\n",
        "    tf.compat.v1.logging.info(\"Start generating the dialogue examples.\")\n",
        "    _create_dialog_examples(processor, dial_file)\n",
        "    tf.compat.v1.logging.info(\"Finish generating the dialogue examples.\")\n",
        "\n",
        "    # Generate the schema embeddings if needed or specified.\n",
        "    bert_init_ckpt = os.path.join(FLAGS.bert_ckpt_dir, \"bert_model.ckpt\")\n",
        "    tokenization.validate_case_matches_checkpoint(\n",
        "      do_lower_case=FLAGS.do_lower_case, init_checkpoint=bert_init_ckpt)\n",
        "\n",
        "    bert_config = modeling.BertConfig.from_json_file(\n",
        "      os.path.join(FLAGS.bert_ckpt_dir, \"bert_config.json\"))\n",
        "    if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n",
        "    raise ValueError(\n",
        "        \"Cannot use sequence length %d because the BERT model \"\n",
        "        \"was only trained up to sequence length %d\" %\n",
        "        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n",
        "\n",
        "    schema_embedding_file = os.path.join(\n",
        "      FLAGS.schema_embedding_dir,\n",
        "      \"{}_pretrained_schema_embedding.npy\".format(FLAGS.dataset_split))\n",
        "    if (not tf.io.gfile.exists(schema_embedding_file) or\n",
        "      FLAGS.overwrite_schema_emb_file):\n",
        "    tf.compat.v1.logging.info(\"Start generating the schema embeddings.\")\n",
        "    _create_schema_embeddings(bert_config, schema_embedding_file,\n",
        "                              dataset_config)\n",
        "    tf.compat.v1.logging.info(\"Finish generating the schema embeddings.\")\n",
        "\n",
        "    # Create estimator for training or inference.\n",
        "    tf.io.gfile.makedirs(FLAGS.output_dir)\n",
        "\n",
        "    tpu_cluster_resolver = None\n",
        "    if FLAGS.use_tpu and FLAGS.tpu_name:\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
        "\n",
        "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "    run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      master=FLAGS.master,\n",
        "      model_dir=FLAGS.output_dir,\n",
        "      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
        "      keep_checkpoint_max=None,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          # Recommended value is number of global steps for next checkpoint.\n",
        "          iterations_per_loop=FLAGS.save_checkpoints_steps,\n",
        "          num_shards=FLAGS.num_tpu_cores,\n",
        "          per_host_input_for_training=is_per_host))\n",
        "\n",
        "    num_train_steps = None\n",
        "    num_warmup_steps = None\n",
        "    if FLAGS.run_mode == \"train\":\n",
        "    num_train_examples = processor.get_num_dialog_examples(FLAGS.dataset_split)\n",
        "    num_train_steps = int(num_train_examples / FLAGS.train_batch_size *\n",
        "                          FLAGS.num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
        "\n",
        "    bert_init_ckpt = os.path.join(FLAGS.bert_ckpt_dir, \"bert_model.ckpt\")\n",
        "    model_fn = _model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=bert_init_ckpt,\n",
        "      learning_rate=FLAGS.learning_rate,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=FLAGS.use_tpu,\n",
        "      use_one_hot_embeddings=FLAGS.use_tpu)\n",
        "\n",
        "    # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "    # or GPU.\n",
        "    estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=FLAGS.use_tpu,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=FLAGS.train_batch_size,\n",
        "      eval_batch_size=FLAGS.eval_batch_size,\n",
        "      predict_batch_size=FLAGS.predict_batch_size)\n",
        "\n",
        "    if FLAGS.run_mode == \"train\":\n",
        "    # Train the model.\n",
        "    tf.compat.v1.logging.info(\"***** Running training *****\")\n",
        "    tf.compat.v1.logging.info(\"  Num dial examples = %d\", num_train_examples)\n",
        "    tf.compat.v1.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
        "    tf.compat.v1.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "    train_input_fn = _file_based_input_fn_builder(\n",
        "        dataset_config=dataset_config,\n",
        "        input_dial_file=dial_file,\n",
        "        schema_embedding_file=schema_embedding_file,\n",
        "        is_training=True,\n",
        "        drop_remainder=True)\n",
        "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "    elif FLAGS.run_mode == \"predict\":\n",
        "    # Run inference to obtain model predictions.\n",
        "    num_actual_predict_examples = processor.get_num_dialog_examples(\n",
        "        FLAGS.dataset_split)\n",
        "\n",
        "    tf.compat.v1.logging.info(\"***** Running prediction *****\")\n",
        "    tf.compat.v1.logging.info(\"  Num actual examples = %d\",\n",
        "                              num_actual_predict_examples)\n",
        "    tf.compat.v1.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
        "\n",
        "    predict_input_fn = _file_based_input_fn_builder(\n",
        "        dataset_config=dataset_config,\n",
        "        input_dial_file=dial_file,\n",
        "        schema_embedding_file=schema_embedding_file,\n",
        "        is_training=False,\n",
        "        drop_remainder=FLAGS.use_tpu)\n",
        "\n",
        "    input_json_files = [\n",
        "        os.path.join(FLAGS.dstc8_data_dir, FLAGS.dataset_split,\n",
        "                     \"dialogues_{:03d}.json\".format(fid))\n",
        "        for fid in dataset_config.file_ranges[FLAGS.dataset_split]\n",
        "    ]\n",
        "    schema_json_file = os.path.join(FLAGS.dstc8_data_dir, FLAGS.dataset_split,\n",
        "                                    \"schema.json\")\n",
        "\n",
        "    ckpt_nums = [num for num in FLAGS.eval_ckpt.split(\",\") if num]\n",
        "    if not ckpt_nums:\n",
        "        raise ValueError(\"No checkpoints assigned for prediction.\")\n",
        "    for ckpt_num in ckpt_nums:\n",
        "        tf.compat.v1.logging.info(\"***** Predict results for %s set *****\",\n",
        "                                FLAGS.dataset_split)\n",
        "\n",
        "        predictions = estimator.predict(\n",
        "          input_fn=predict_input_fn,\n",
        "          checkpoint_path=os.path.join(FLAGS.output_dir,\n",
        "                                       \"model.ckpt-%s\" % ckpt_num))\n",
        "\n",
        "        # Write predictions to file in DSTC8 format.\n",
        "        dataset_mark = os.path.basename(FLAGS.dstc8_data_dir)\n",
        "        prediction_dir = os.path.join(\n",
        "          FLAGS.output_dir, \"pred_res_{}_{}_{}_{}\".format(\n",
        "              int(ckpt_num), FLAGS.dataset_split, task_name, dataset_mark))\n",
        "        if not tf.io.gfile.exists(prediction_dir):\n",
        "        tf.io.gfile.makedirs(prediction_dir)\n",
        "        pred_utils.write_predictions_to_file(predictions, input_json_files,\n",
        "                                           schema_json_file, prediction_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfu_1rulGONa",
        "colab_type": "text"
      },
      "source": [
        "# 10. Running."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4W5M6nZRGONb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "flags.mark_flag_as_required(\"dstc8_data_dir\")\n",
        "flags.mark_flag_as_required(\"bert_ckpt_dir\")\n",
        "flags.mark_flag_as_required(\"dataset_split\")\n",
        "flags.mark_flag_as_required(\"schema_embedding_dir\")\n",
        "flags.mark_flag_as_required(\"dialogues_example_dir\")\n",
        "flags.mark_flag_as_required(\"task_name\")\n",
        "flags.mark_flag_as_required(\"output_dir\")\n",
        "tf.compat.v1.app.run(main)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}